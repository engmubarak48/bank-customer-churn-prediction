{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author: [Juste Nyirimana](https://www.linkedin.com/in/juste-nyirimana-25a534144/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UmojaHack Rwanda: Expresso Churn Prediction Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When customers leave, this can be very costly for the company, thus firms are interested in predicting churn beforehand. Having that information in hand can help companies change their strategy to retain customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this starter notebook, we'll walk through the competition. We will show you how to load the data and do a quick exploratory analysis. Then, we will train a simple model, make some predictions, and then submit those predictions to the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have describes 2.5 million [Expresso](https://www.expressotelecom.sn/) clients. Expresso is an African telecommunications services company that provides telecommunication services in five African markets: Mauritania, Senegal, Guinea, and Ghana. Expresso offers a wide range of products and services to meet the needs of customers.\n",
    "\n",
    "The objective of this hackathon is to develop a predictive model that determines the likelihood for a customer to churn - to stop purchasing airtime and data from Expresso. Let's dig in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "&nbsp;&nbsp;1. [LOADING THE DATA](#1)\n",
    "   \n",
    "&nbsp;&nbsp;2. [EXPLORING THE DATA](#2)   \n",
    "\n",
    "&nbsp;&nbsp;3. [BUILDING SOME MODELS](#3)   \n",
    "\n",
    "&nbsp;&nbsp;4. [GENERATING THE PREDICTIONS FOR THE TEST SET](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the [data description page](https://zindi.africa/hackathons/umojahack-rwanda-expresso-churn-prediction-challenge/data), we are provided with everything we need to get started:\n",
    "\n",
    "Train.csv: this is what you will use to train your model.\n",
    "\n",
    "Test.csv: this is what you will test your model on.\n",
    "\n",
    "SampleSubmission.csv: This file serves as an example for how to format your submission.\n",
    "\n",
    "Let's start by importing the libraries that we will need to load and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing packages\n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can load the datasets and begin taking a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data\n",
    "train = pd.read_csv('UmojaHack/Train.csv')\n",
    "test = pd.read_csv('UmojaHack/Test.csv')\n",
    "submission_file = pd.read_csv('UmojaHack/SampleSubmission.csv')\n",
    "describtion = pd.read_csv('UmojaHack/VariableDefinitions.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is/was an expresso client. The columns are the different attributes of the customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()#quite the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than 2M customers in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data[missing_data.Total > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = test.isnull().sum().sort_values(ascending=False)\n",
    "percent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data[missing_data.Total > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(exclude = np.number)#no duplicates in user_id, MRG is useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe(exclude = np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 'MRG' column since it consists of a single value. Also, let's delete colums 'ZONE2' and 'ZONE1' since they have so many missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.REGION.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print()\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the distribution of our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.groupby(['CHURN']).size())\n",
    "#bar chart to show distribution of the target variable\n",
    "n_obs = train.shape[0]\n",
    "#index = ['No','Yes']\n",
    "churn_plot = train['CHURN'].value_counts().div(n_obs).plot(kind='bar',figsize=(4,4),title=\"Churn rate\", color=['#BB6B5A','#8CCB9B'])\n",
    "churn_plot.set_xlabel(\"Churn Risk\")\n",
    "churn_plot.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is imbalanced, with more than 80% of customers staying. Next, let's take a look at our features. We have a mix of continous and categorical features. \n",
    "\n",
    "Let's begin with the categorical features and investigate how the churn rate differ across the various levels of each categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attrition_rate_plot(col, target, data, ax=None):\n",
    "    \"\"\"Stacked bar chart of churn rate for `target` against \n",
    "    `col`. \n",
    "    \n",
    "    Args:\n",
    "        col (string): column name of feature variable\n",
    "        target (string): column name of target variable\n",
    "        data (pandas DataFrame): dataframe that contains columns \n",
    "            `col` and `target`\n",
    "        ax (matplotlib axes object, optional): matplotlib axes \n",
    "            object to attach plot to\n",
    "    \"\"\"\n",
    "    counts = (train[[target, col]]\n",
    "                  .groupby([target, col])\n",
    "                  .size()\n",
    "                  .unstack(target)\n",
    "             )\n",
    "    group_counts = counts.sum(axis='columns')\n",
    "    props = counts.div(group_counts, axis='index')\n",
    "\n",
    "    props.plot(kind=\"barh\", stacked=True, ax=ax, color = ['g', 'r'])\n",
    "    ax.invert_yaxis()\n",
    "    ax.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = [\n",
    "    'REGION',\n",
    "    'TENURE'\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(cols_to_plot), 1, figsize=(11,11)\n",
    ")\n",
    "for idx, col in enumerate(cols_to_plot):\n",
    "    attrition_rate_plot(\n",
    "        col, 'CHURN', train, ax=ax[idx]\n",
    "    )\n",
    "\n",
    "ax[0].legend(\n",
    "    loc='lower center', bbox_to_anchor=(0.5, 1.05), title='Churn'\n",
    ")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like customers with tenure between 12 and 15 months have the highest churn rate while new customers have the lowest.\n",
    "\n",
    "Now, let's look at the relationship between the recharge amount and the monthly revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot options\n",
    "sns.set_style('white')\n",
    "plt.figure(figsize = (14, 8))\n",
    "\n",
    "# Create scatterplot\n",
    "sns.scatterplot(x = \"MONTANT\", \n",
    "                y = \"REVENUE\", \n",
    "                # Group by and change dot style and  by CHURN\n",
    "                hue = \"CHURN\",\n",
    "                size = \"CHURN\",  \n",
    "                style = \"CHURN\", \n",
    "                data = train, \n",
    "                # Change color of hue categories\n",
    "                palette = [\"r\", \"g\"],\n",
    "                alpha = 0.2)\n",
    "\n",
    "# Despine plot\n",
    "sns.despine()\n",
    "# Final formatting touches\n",
    "plt.xlabel(\"MONTANT\", fontsize = 12, fontweight = \"semibold\")\n",
    "plt.ylabel(\"REVENUE\", fontsize = 12, fontweight = \"semibold\")\n",
    "plt.title(\"REVENUE by AMOUNT\", fontsize = 14, fontweight = \"semibold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some correlation between the two and also there's some outliers. Outliers can create bias in a model's performance. Should we keep them or remove them? I will let you decide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING SOME MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model using the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). We chose the logistic regression because it is fast and serve as a good baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "RANDOM_SEED = 123    # Set a random seed for reproducibility!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models, including logistic regression, only work with numeric input for features. So we'll either have to drop the categorical features or transform them. We'll opt for the later since we want to capture as much as information as we can. To do this, we'll use a method called [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to do some preprocessing for the numeric features. We have to scale each numeric feature and the reason for that is because we are using [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)). We will use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), it transforms each feature such that its distribution has a mean value of 0 and a standard deviation of 1.\n",
    "\n",
    "Another issue we have to take care of is the missing values since logistic regression can't handle them. For that, we'll use theSimpleImputer (https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) function.\n",
    "\n",
    "This is quite a lot of steps but not to worry, we'll use Scikit-Learn's built-in composition functionality to encapsulate everything into a pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data\n",
    "train = pd.read_csv('UmojaHack/Train.csv')\n",
    "test = pd.read_csv('UmojaHack/Test.csv')\n",
    "submission_file = pd.read_csv('UmojaHack/SampleSubmission.csv')\n",
    "describtion = pd.read_csv('UmojaHack/VariableDefinitions.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# preprocess = pd.DataFrame()\n",
    "\n",
    "# #preprocess['TOP_PACK'] = train.TOP_PACK\n",
    "\n",
    "# to_extracts = {\n",
    "#    'p_unlimited':r'unlimited|ILLIMITE',\n",
    "#    'p_onnet':r'On ?net',\n",
    "#    'p_offnet':r'Off',\n",
    "#    'p_pilot':r'pilot',\n",
    "#    'p_jokko':r'JOKKO',\n",
    "#    'p_social':r'FACEBOOK|TWTER',\n",
    "#    'p_week':r'WEEK|7',\n",
    "#    'p_month':r'MONTH|30',\n",
    "# }\n",
    "# for name, exp in to_extracts.items():\n",
    "#     filt = train.TOP_PACK.str.contains(exp, flags=re.IGNORECASE, na=False)\n",
    "#     preprocess[name] = filt * 1\n",
    "\n",
    "# preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.drop(['MRG', 'ZONE2', 'ZONE1'], axis = 1)\n",
    "# test = test.drop(['MRG', 'ZONE2', 'ZONE1'], axis = 1)\n",
    "# train['ZONE'] = train[['ZONE1','ZONE2']].sum(axis=1)\n",
    "# test['ZONE'] = test[['ZONE1','ZONE2']].sum(axis=1)\n",
    "\n",
    "train = train.drop(['user_id', 'MRG', 'ZONE2', 'ZONE1'], axis = 1)\n",
    "test = test.drop(['user_id', 'MRG', 'ZONE2', 'ZONE1'], axis = 1)\n",
    "\n",
    "# def prep(train):\n",
    "\n",
    "feat_operator = train[['ON_NET', 'ORANGE', 'TIGO']]\n",
    "feat_operator = feat_operator.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "train[['ON_NET', 'ORANGE', 'TIGO']] = feat_operator\n",
    "#------------------------------------------\n",
    "columns_means = ['MONTANT', 'FREQUENCE_RECH', 'FREQUENCE', 'REVENUE', 'FREQ_TOP_PACK']\n",
    "\n",
    "for column_mean in columns_means:\n",
    "    mean_rev = round(np.log(train[[column_mean]].mean()[0]), 2)\n",
    "    train[column_mean] = np.log(train[[column_mean]].fillna(mean_rev))\n",
    "#-------------------------------------\n",
    "mean_rev = round(np.log(1+train[['ARPU_SEGMENT']].mean()[0]), 2)\n",
    "train['ARPU_SEGMENT'] = np.log(1+train[['ARPU_SEGMENT']].fillna(mean_rev))\n",
    "\n",
    "train['REGION'] = train[['REGION']].fillna('unknown')\n",
    "\n",
    "#     fill_list = list(train[['DATA_VOLUME']].dropna().values.reshape(-1))\n",
    "\n",
    "#     m = train['DATA_VOLUME'].isnull()\n",
    "#     #set NaNs values\n",
    "#     train.loc[m, 'DATA_VOLUME'] = np.random.choice(fill_list, size=m.sum())\n",
    "#     train['DATA_VOLUME'] = np.log(1+train[['DATA_VOLUME']])\n",
    "#     return train\n",
    "\n",
    "# train = prep(train)\n",
    "# test = prep(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.columns[train.dtypes != \"object\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MONTANT', 'FREQUENCE_RECH', 'REVENUE', 'ARPU_SEGMENT', 'FREQUENCE', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO', 'REGULARITY', 'FREQ_TOP_PACK']\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = ['MONTANT', 'FREQUENCE_RECH', 'REVENUE', 'ARPU_SEGMENT',\n",
    "       'FREQUENCE', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO',\n",
    "       'REGULARITY', 'FREQ_TOP_PACK']\n",
    "print(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['REGION', 'TENURE', 'TOP_PACK']\n"
     ]
    }
   ],
   "source": [
    "#categorical_cols = df.columns[df.dtypes == \"object\"].values\n",
    "categorical_cols = ['REGION', 'TENURE', 'TOP_PACK']\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain preprocessing into a Pipeline object\n",
    "# each step is a tuple of (name you chose, sklearn transformer)\n",
    "numeric_preprocessing_steps = Pipeline([\n",
    "    ('standard_scaler', StandardScaler()),\n",
    "    ('simple_imputer', SimpleImputer(strategy='constant', fill_value=0, add_indicator = True))\n",
    "])\n",
    "\n",
    "categorical_preprocessing_steps = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy='constant', fill_value='Missing', add_indicator = True)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# create the preprocessor stage of final pipeline\n",
    "# each entry in the transformer list is a tuple of\n",
    "# (name you choose, sklearn transformer, list of columns)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"numeric\", numeric_preprocessing_steps, numeric_cols),\n",
    "        ('categorical', categorical_preprocessing_steps, categorical_cols)\n",
    "    ],\n",
    "    remainder = \"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Together the Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put both the preprocessing functions and the estimatior into one Pipeline object, this allows to run the data through all the steps in one interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "full_pipeline_LGBM = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimator\", LGBMClassifier()\n",
    "),\n",
    "])\n",
    "\n",
    "full_pipeline_XGB = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimator\", XGBClassifier(max_depth=20)\n",
    "),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGION</th>\n",
       "      <th>TENURE</th>\n",
       "      <th>MONTANT</th>\n",
       "      <th>FREQUENCE_RECH</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>ARPU_SEGMENT</th>\n",
       "      <th>FREQUENCE</th>\n",
       "      <th>DATA_VOLUME</th>\n",
       "      <th>ON_NET</th>\n",
       "      <th>ORANGE</th>\n",
       "      <th>TIGO</th>\n",
       "      <th>REGULARITY</th>\n",
       "      <th>TOP_PACK</th>\n",
       "      <th>FREQ_TOP_PACK</th>\n",
       "      <th>CHURN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FATICK</td>\n",
       "      <td>K &gt; 24 month</td>\n",
       "      <td>4250.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>4251.000000</td>\n",
       "      <td>1417.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>On net 200F=Unlimited _call24H</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>I 18-21 month</td>\n",
       "      <td>5532.116998</td>\n",
       "      <td>11.52912</td>\n",
       "      <td>5510.810334</td>\n",
       "      <td>1836.942894</td>\n",
       "      <td>13.978141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.272461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>unknown</td>\n",
       "      <td>K &gt; 24 month</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17</td>\n",
       "      <td>On-net 1000F=10MilF;10d</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DAKAR</td>\n",
       "      <td>K &gt; 24 month</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>13502.000000</td>\n",
       "      <td>4501.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>43804.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>62</td>\n",
       "      <td>Data:1000F=5GB,7d</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>DAKAR</td>\n",
       "      <td>K &gt; 24 month</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>Mixt 250F=Unlimited_call24H</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    REGION         TENURE       MONTANT  FREQUENCE_RECH       REVENUE  \\\n",
       "0   FATICK   K > 24 month   4250.000000        15.00000   4251.000000   \n",
       "1  unknown  I 18-21 month   5532.116998        11.52912   5510.810334   \n",
       "2  unknown   K > 24 month   3600.000000         2.00000   1020.000000   \n",
       "3    DAKAR   K > 24 month  13500.000000        15.00000  13502.000000   \n",
       "4    DAKAR   K > 24 month   1000.000000         1.00000    985.000000   \n",
       "\n",
       "   ARPU_SEGMENT  FREQUENCE  DATA_VOLUME  ON_NET  ORANGE  TIGO  REGULARITY  \\\n",
       "0   1417.000000  17.000000          4.0   388.0    46.0   1.0          54   \n",
       "1   1836.942894  13.978141          NaN     0.0     1.0   1.0           4   \n",
       "2    340.000000   2.000000          NaN    90.0    46.0   7.0          17   \n",
       "3   4501.000000  18.000000      43804.0    41.0   102.0   2.0          62   \n",
       "4    328.000000   1.000000          NaN    39.0    24.0   1.0          11   \n",
       "\n",
       "                         TOP_PACK  FREQ_TOP_PACK  CHURN  \n",
       "0  On net 200F=Unlimited _call24H       8.000000      0  \n",
       "1                             NaN       9.272461      1  \n",
       "2         On-net 1000F=10MilF;10d       1.000000      0  \n",
       "3               Data:1000F=5GB,7d      11.000000      0  \n",
       "4     Mixt 250F=Unlimited_call24H       2.000000      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import set_config\n",
    "\n",
    "# full_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into a training and evaluation set. We'll use a third of our data for evaluation. \n",
    "\n",
    "Recall that we have an imbalanced dataset, so we'll use the stratify argument to enforce even splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    train.drop(['CHURN'], axis = 1),\n",
    "    train.CHURN,\n",
    "    test_size=0.20,\n",
    "    shuffle=True,\n",
    "    stratify=train.CHURN,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train model\n",
    "full_pipeline_XGB.fit(X_train, y_train)\n",
    "\n",
    "# Predict on evaluation set\n",
    "# This competition wants probabilities, not labels\n",
    "preds_XGB = full_pipeline_XGB.predict_proba(X_eval)\n",
    "preds_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_eval, preds_XGB[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 292 ms, total: 28.9 s\n",
      "Wall time: 28.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99580003, 0.00419997],\n",
       "       [0.94133634, 0.05866366],\n",
       "       [0.79959716, 0.20040284],\n",
       "       ...,\n",
       "       [0.99408769, 0.00591231],\n",
       "       [0.99822307, 0.00177693],\n",
       "       [0.92976585, 0.07023415]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train model\n",
    "full_pipeline_LGBM.fit(X_train, y_train)\n",
    "\n",
    "# Predict on evaluation set\n",
    "# This competition wants probabilities, not labels\n",
    "preds_LGBM = full_pipeline_LGBM.predict_proba(X_eval)\n",
    "preds_LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_eval, preds_LGBM[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = 1/2*(preds_LGBM[:, 1]+preds_LGBM[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_eval, new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# print(classification_report(np.array(y_eval), preds[:, 1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition uses the [Log loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) as the evaluation metric, so let's check how our model did on the evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimator spits out the probabilities for each class(0 and 1). We are interested in the second column, that is the probability of churn,so let's grab it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy as sp\n",
    "# import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Confusion matrix:\\n', skplt.metrics.plot_confusion_matrix(y_eval, preds[:, 1].round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Model on Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of how the model performs, let's retrain on the full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# full_pipeline.fit(train.drop(['user_id', 'CHURN'], axis = 1), train.CHURN)\n",
    "\n",
    "# None   # So we don't print out the whole pipeline representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATING THE PREDICTIONS FOR THE TEST SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions on the test set! Remember, for this competition, we want the probabilities, not the binary label predictions. So just like we did earlier, we'll use the .predict_proba method to get those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feat_operator = test[['ON_NET', 'ORANGE', 'TIGO']]\n",
    "# feat_operator = feat_operator.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "# test[['ON_NET', 'ORANGE', 'TIGO']] = feat_operator\n",
    "# #------------------------------------------\n",
    "# columns_means = ['MONTANT', 'FREQUENCE_RECH', 'FREQUENCE', 'REVENUE', 'FREQ_TOP_PACK']\n",
    "\n",
    "# for column_mean in columns_means:\n",
    "#     mean_rev = round(np.log(test[[column_mean]].mean()[0]), 2)\n",
    "#     test[column_mean] = np.log(test[[column_mean]].fillna(mean_rev))\n",
    "# #-------------------------------------\n",
    "# mean_rev = round(np.log(1+test[['ARPU_SEGMENT']].mean()[0]), 2)\n",
    "# test['ARPU_SEGMENT'] = np.log(1+test[['ARPU_SEGMENT']].fillna(mean_rev))\n",
    "\n",
    "# test['REGION'] = test[['REGION']].fillna('unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probas_xgb = full_pipeline_XGB.predict_proba(test)\n",
    "test_probas_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probas_LGBM = full_pipeline_LGBM.predict_proba(test)\n",
    "test_probas_LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = 1/2*(test_probas_xgb[:, 1]+test_probas_LGBM[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the submission file to submit the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('UmojaHack/SampleSubmission.csv')\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to replace those 0s with our predictions. But first, we need to make sure that the rows of the submission file are in the same order as the test file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have the rows in the same order\n",
    "np.testing.assert_array_equal(test.index.values, \n",
    "                              submission_df.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happended, so we're good. We can safely can drop in the estimated values in the 'CHURN' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to submission data frame\n",
    "submission_df[\"CHURN\"] = new_preds #test_probas_xgb[:, 1]\n",
    "\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('my_submission_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head my_submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's on head on over to [Zindi](https://zindi.africa/hackathons/umojahack-rwanda-expresso-churn-prediction-challenge) and make our submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to find optimal hyper parameters. If you do chose to use gridsearch, I suggest splitting the dataset into small portions since the it is so large\n",
    "* Mean target encoding\n",
    "* [Principal component analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) (PCA)\n",
    "* Removing outliers\n",
    "* Try different algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
